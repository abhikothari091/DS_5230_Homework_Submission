{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d53b851-2cb4-4d1e-8d5c-d7637ac1aa31",
   "metadata": {},
   "source": [
    "# Name: Abhishek Kothari\n",
    "# Assignment 2A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68092355-b1a8-4852-a5d2-272897673bcc",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958870a9-32b9-4929-860f-0581e3a2c160",
   "metadata": {},
   "source": [
    "Proof for KMeans Objective\n",
    "\n",
    "Part A: E-step minimizes the objective given current centroids ($(\\mu)$)\n",
    "\n",
    "The KMeans objective is defined as:\n",
    "\n",
    "$$\n",
    "J(\\pi, \\mu) = min(\\sum_{i=1}^n \\sum_{k=1}^K \\pi_{ik} \\|x_i - \\mu_k\\|^2)\n",
    "$$\n",
    "\n",
    "Step 1: Fix centroids ($(\\mu)$)\n",
    "In the E-step, we update the memberships ($(\\pi_{ik})$) for each data point $(x_i)$, given fixed centroids $(\\mu_k)$.\n",
    "\n",
    "Step 2: Objective for a single point\n",
    "For a fixed $(x_i)$, the objective simplifies to:\n",
    "\n",
    "$$\n",
    "\\sum_{k=1}^K \\pi_{ik} |x_i - \\mu_k|^2\n",
    "$$\n",
    "\n",
    "Here, $(\\pi_{ik} \\in \\{0, 1\\}) and (\\sum_{k=1}^K \\pi_{ik} = 1)$. The objective is minimized when:\n",
    "\n",
    "$$\n",
    "\\pi_{ik} = \n",
    "\\begin{cases} \n",
    "1 & \\text{if } k = \\arg\\min_j \\|x_i - \\mu_j\\|^2 \\\\\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This assigns $(x_i)$ to the closest centroid, minimizing the distance.\n",
    "\n",
    "Conclusion:\n",
    "Updating $(\\pi_{ik})$ as described minimizes the objective for fixed centroids.\n",
    "\n",
    "---\n",
    "\n",
    "Part B: M-step minimizes the objective given current memberships ($(\\pi)$)\n",
    "\n",
    "Step 1: Fix memberships ($(\\pi_{ik})$)\n",
    "In the M-step, we update the centroids ($(\\mu_k)$) for each cluster $(k)$, given fixed memberships $(\\pi_{ik})$.\n",
    "\n",
    "Step 2: Objective for a single centroid\n",
    "For a fixed cluster $(k)$, the objective simplifies to:\n",
    "\n",
    "$$\n",
    "\\sum_{i: \\pi_{ik}=1} \\|x_i - \\mu_k\\|^2\n",
    "$$\n",
    "\n",
    "This is a sum of squared distances between the centroid $(\\mu_k)$ and the points assigned to cluster \\(k\\).\n",
    "\n",
    "Step 3:\n",
    "The optimal $(\\mu_k)$ is the point that minimizes the sum of squared distances. This is the mean of the points in cluster $(k)$:\n",
    "\n",
    "$$\n",
    "\\mu_k = \\frac{\\sum_{i: \\pi_{ik}=1} x_i}{\\sum_{i: \\pi_{ik}=1} 1}\n",
    "$$\n",
    "\n",
    "Conclusion:\n",
    "Updating $(\\mu_k)$ as the mean of points minimizes the objective for fixed memberships.\n",
    "\n",
    "---\n",
    "\n",
    "Part C: Why does KMeans converge but not necessarily to the global minimum?\n",
    "\n",
    "Step 1: Objective decreases monotonically\n",
    "Each E-step and M-step decreases the objective function or leaves it unchanged. The KMeans objective $(J(\\pi, \\mu))$ is non-negative as its a distance and bounded below by 0.\n",
    "\n",
    "Step 2: Convergence to a local minimum\n",
    "Since $(J(\\pi, \\mu))$ decreases monotonically and is bounded, KMeans must converge to a value where further updates do not change the objective. This is typically a local minimum of the objective.\n",
    "\n",
    "Step 3: Not guaranteed to find the global minimum\n",
    "The KMeans algorithm is sensitive to the initial centroids. Poor initialization of these centroids can lead to convergence at suboptimal (local) minima.\n",
    "\n",
    "Conclusion:\n",
    "KMeans converges due to the monotonic decrease of the objective, but it does not guarantee finding the global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5c9200-f71b-47ec-8818-d5e5b9a4bab9",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d772eecb-ee5f-4acf-b78e-02986949496a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b19ace8f-aa81-4b02-a1ad-1969555ceb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    def __init__(self, n_clusters=10, max_iterations=300, metric='euclidean'):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iterations = max_iterations\n",
    "        self.metric = metric\n",
    "        self.centroids = None\n",
    "        self.labels = None\n",
    "\n",
    "    def fit(self, x):\n",
    "        n_samples, n_features = x.shape  \n",
    "    # here we first randomly assign centroids\n",
    "        np.random.seed(42)\n",
    "        self.centroids = x[np.random.choice(n_samples, self.n_clusters, replace=False)]\n",
    "        self.labels = np.zeros(n_samples, dtype=int)  \n",
    "\n",
    "        for _ in range(self.max_iterations):\n",
    "            distances = pairwise_distances(x, self.centroids, metric=self.metric)\n",
    "            new_labels = np.argmin(distances, axis=1)\n",
    "            if np.all(self.labels == new_labels):\n",
    "                break\n",
    "            self.labels = new_labels\n",
    "\n",
    "            for k in range(self.n_clusters):\n",
    "                cluster_points = x[self.labels == k]\n",
    "                if len(cluster_points) > 0:\n",
    "                    self.centroids[k] = np.mean(cluster_points, axis=0)\n",
    "        return self\n",
    "\n",
    "    def inertia(self, X):\n",
    "        # sum of sq dist\n",
    "        distances = pairwise_distances(X, self.centroids, metric=self.metric)\n",
    "        return np.sum(np.min(distances, axis=1)**2)\n",
    "                \n",
    "    def predict(self, X):\n",
    "        # assigns labels to new data\n",
    "        distances = pairwise_distances(X, self.centroids, metric=self.metric)\n",
    "        return np.argmin(distances, axis=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_purity_gini(labels, true_labels):\n",
    "        n_samples = len(true_labels)\n",
    "        clusters = np.unique(labels)\n",
    "        purity, gini = 0, 0\n",
    "\n",
    "        for cluster in clusters:\n",
    "            indices = np.where(labels == cluster)[0]\n",
    "            cluster_labels = true_labels[indices]\n",
    "            most_common_label, count = Counter(cluster_labels).most_common(1)[0]\n",
    "            purity += count\n",
    "\n",
    "            label_counts = Counter(cluster_labels)\n",
    "            gini_k = 1 - sum((count / len(indices))**2 for count in label_counts.values())\n",
    "            gini += gini_k * len(indices)\n",
    "\n",
    "        purity /= n_samples\n",
    "        gini /= n_samples\n",
    "        return purity, gini\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7bc412-f7a3-4f85-bd7b-886379d2043b",
   "metadata": {},
   "source": [
    "Task 1) Mnist Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ab1080ce-07c2-4ed4-aeb8-6944b9968d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purity for mnist for k=10 is: 0.5286\n",
      "The gini for mnist for k=10 is: 0.6100\n",
      "The inertia for MNIST with k = 10 is: 2364716.07\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_images = train_images / 255.0\n",
    "train_images = train_images.reshape(len(train_images), -1)\n",
    "k = 10\n",
    "kmeans = KMeans(n_clusters=k, max_iterations=300, metric='euclidean')\n",
    "kmeans.fit(train_images)\n",
    "\n",
    "purity, gini = KMeans.calculate_purity_gini(kmeans.labels, train_labels)\n",
    "print(f\"The purity for mnist for k=10 is: {purity:.4f}\")\n",
    "print(f\"The gini for mnist for k=10 is: {gini:.4f}\")\n",
    "\n",
    "inertia = kmeans.inertia(train_images)\n",
    "print(f\"The inertia for MNIST with k = 10 is: {inertia:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "62623c42-e73f-41ae-932f-67df5b7754a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purity for mnist for k=20 is: 0.7091\n",
      "The gini for mnist for k=20 is: 0.3974\n",
      "The inertia for MNIST with k = 20 is: 2111960.40\n"
     ]
    }
   ],
   "source": [
    "k = 20\n",
    "kmeans = KMeans(n_clusters=k, max_iterations=300, metric='euclidean')\n",
    "kmeans.fit(train_images)\n",
    "\n",
    "purity, gini = KMeans.calculate_purity_gini(kmeans.labels, train_labels)\n",
    "print(f\"The purity for mnist for k=20 is: {purity:.4f}\")\n",
    "print(f\"The gini for mnist for k=20 is: {gini:.4f}\")\n",
    "\n",
    "inertia = kmeans.inertia(train_images)\n",
    "print(f\"The inertia for MNIST with k = 20 is: {inertia:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "325c8f33-30d9-41e1-96fd-351589838424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purity for mnist for k=5 is: 0.4523\n",
      "The gini for mnist for k=5 is: 0.6585\n",
      "The inertia for MNIST with k = 5 is: 2605960.41\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "kmeans = KMeans(n_clusters=k, max_iterations=300, metric='euclidean')\n",
    "kmeans.fit(train_images)\n",
    "\n",
    "purity, gini = KMeans.calculate_purity_gini(kmeans.labels, train_labels)\n",
    "print(f\"The purity for mnist for k=5 is: {purity:.4f}\")\n",
    "print(f\"The gini for mnist for k=5 is: {gini:.4f}\")\n",
    "\n",
    "inertia = kmeans.inertia(train_images)\n",
    "print(f\"The inertia for MNIST with k = 5 is: {inertia:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bde99b-9e53-4546-b1c8-815873aef077",
   "metadata": {},
   "source": [
    "Task 2) Fashion Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca3c9d1c-e59c-4c48-8608-fd3e833d5a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12028f22-930a-459d-85ec-4094d3baaf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train),(x_test,y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "x_data = np.concatenate((x_train,x_test),axis = 0)\n",
    "y_data = np.concatenate((y_train,y_test), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff838c68-162d-452b-bed8-f56a52a2f7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = x_data/255.0\n",
    "x_data = x_data.reshape(x_data.shape[0],-1)\n",
    "x_data = normalize(x_data,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f99f776c-a8c9-4a13-8669-2bed986fbc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (70000, 784)\n",
      "Labels shape: (70000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset shape:\", x_data.shape)  \n",
    "print(\"Labels shape:\", y_data.shape)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cdbc5cc1-a84a-4645-9874-d4b10f04050a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.KMeans at 0x3296bf110>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 10\n",
    "kmeans_f = KMeans(n_clusters = k)\n",
    "kmeans_f.fit(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "79f25883-afad-4ae3-8f53-d856f2650a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purity of fashion data for k = 10 is: 0.5675428571428571\n",
      "The gini of fashion data for k = 10 is: 0.5411670406959518\n"
     ]
    }
   ],
   "source": [
    "purity, gini = kmeans_f.calculate_purity_gini(kmeans_f.labels, y_data)\n",
    "print(f\"The purity of fashion data for k = 10 is: {purity}\")\n",
    "print(f\"The gini of fashion data for k = 10 is: {gini}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1967e6cf-ff27-4ec3-acb9-9bd6957038c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The inertia for fashion data with k = 10 is: 14931.30489011417\n"
     ]
    }
   ],
   "source": [
    "inertia = kmeans_f.inertia(x_data)\n",
    "print(f\"The inertia for fashion data with k = 10 is: {inertia}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "55b323c4-efd7-4119-bf91-f15c5618f117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.KMeans at 0x33e2cdbb0>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 20\n",
    "kmeans_f = KMeans(n_clusters = k)\n",
    "kmeans_f.fit(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "38b838b0-a735-4cfc-aaf7-898408e3de0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purity of fashion data for k = 20 is: 0.6851428571428572\n",
      "The gini of fashion data for k = 20 is: 0.42128678650165496\n"
     ]
    }
   ],
   "source": [
    "purity, gini = kmeans_f.calculate_purity_gini(kmeans_f.labels, y_data)\n",
    "print(f\"The purity of fashion data for k = 20 is: {purity}\")\n",
    "print(f\"The gini of fashion data for k = 20 is: {gini}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2fc8a919-6ce5-49df-a783-e2dc6fba149b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The inertia for fashion data with k = 20 is: 13150.48627279295\n"
     ]
    }
   ],
   "source": [
    "inertia = kmeans_f.inertia(x_data)\n",
    "print(f\"The inertia for fashion data with k = 20 is: {inertia}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "38b33675-af9c-4f91-80bc-f946a92dc213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.KMeans at 0x33e2cc920>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 5\n",
    "kmeans_f = KMeans(n_clusters = k)\n",
    "kmeans_f.fit(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fa071483-edbc-4879-991f-94f98bb1af59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purity of fashion data for k = 5 is: 0.3717285714285714\n",
      "The gini of fashion data for k = 5 is: 0.700740594897485\n"
     ]
    }
   ],
   "source": [
    "purity, gini = kmeans_f.calculate_purity_gini(kmeans_f.labels, y_data)\n",
    "print(f\"The purity of fashion data for k = 5 is: {purity}\")\n",
    "print(f\"The gini of fashion data for k = 5 is: {gini}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f73cdea9-89e4-42bd-8ef4-77232963e595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The inertia for fashion data with k = 5 is: 17515.48710666463\n"
     ]
    }
   ],
   "source": [
    "inertia = kmeans_f.inertia(x_data)\n",
    "print(f\"The inertia for fashion data with k = 5 is: {inertia}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a701a6-040a-4b6a-b070-0c83773797ed",
   "metadata": {},
   "source": [
    "Task 3) 20NG Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb47f030-a9e1-41ef-84e6-15896d99c4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d8694ad-80e1-4b2d-9dea-4ecac82f9afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nggroup = fetch_20newsgroups(subset = 'all', remove=('headers','footers','quotes'))\n",
    "data, labels = nggroup.data, nggroup.target\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "x = vectorizer.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "428dab19-2a51-4e0b-abb6-3c77e0d06578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "\n",
    "class KMeans_ng:\n",
    "    def __init__(self, n_clusters=20, max_iterations=300):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iterations = max_iterations\n",
    "\n",
    "    def fit(self, X):\n",
    "        X = normalize(X, axis=1)\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # randomly init the centroids\n",
    "        self.centroids = X[np.random.choice(n_samples, self.n_clusters, replace=False)].toarray()\n",
    "        self.labels = np.zeros(n_samples, dtype=np.int32)\n",
    "\n",
    "        for _ in range(self.max_iterations):\n",
    "            # dot prod similarity\n",
    "            similarities = X.dot(self.centroids.T)\n",
    "\n",
    "            new_labels = np.argmax(similarities, axis=1)\n",
    "\n",
    "            if np.all(self.labels == new_labels):\n",
    "                break\n",
    "            self.labels = new_labels\n",
    "\n",
    "            for k in range(self.n_clusters):\n",
    "                indices = np.where(self.labels == k)[0]\n",
    "\n",
    "                if len(indices) > 0:\n",
    "                    cluster_points = X[indices]\n",
    "                    # we convert the sparse matrix in a dense row\n",
    "                    self.centroids[k] = cluster_points.mean(axis=0).A1  \n",
    "\n",
    "        return self\n",
    "\n",
    "    def inertia(self, X):\n",
    "        X = normalize(X, axis=1)\n",
    "        similarities = X.dot(self.centroids.T)\n",
    "        return np.sum(np.max(similarities, axis=1))\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = normalize(X, axis=1)\n",
    "        similarities = X.dot(self.centroids.T)\n",
    "        return np.argmax(similarities, axis=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_purity_gini(labels, true_labels):\n",
    "        n_samples = len(true_labels)\n",
    "        clusters = np.unique(labels)\n",
    "        purity, gini = 0, 0\n",
    "\n",
    "        for cluster in clusters:\n",
    "            indices = np.where(labels == cluster)[0]\n",
    "            cluster_labels = true_labels[indices]\n",
    "            most_common_label, count = Counter(cluster_labels).most_common(1)[0]\n",
    "            purity += count\n",
    "\n",
    "            label_counts = Counter(cluster_labels)\n",
    "            gini_k = 1 - sum((count / len(indices))**2 for count in label_counts.values())\n",
    "            gini += gini_k * len(indices)\n",
    "\n",
    "        purity /= n_samples\n",
    "        gini /= n_samples\n",
    "        return purity, gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b1f28d1-4d88-491e-b186-e859b61cbfe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.KMeans_ng at 0x16b2a9ee0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 20\n",
    "kmeans = KMeans_ng(n_clusters = k)\n",
    "kmeans.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "570a03c5-0ce2-459c-add2-05657c00b42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purity of ng group data for k = 20 is: 0.426244295871803\n",
      "The gini of ng group for k = 20 is: 0.7112419326681252\n"
     ]
    }
   ],
   "source": [
    "purity, gini = kmeans.calculate_purity_gini(kmeans.labels, labels)\n",
    "print(f\"The purity of ng group data for k = 20 is: {purity}\")\n",
    "print(f\"The gini of ng group for k = 20 is: {gini}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "50d55078-433c-40f5-9d8d-cf36505293f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The inertia for ng group data with k = 20 is: 400.8554411190797\n"
     ]
    }
   ],
   "source": [
    "inertia = kmeans.inertia(x)\n",
    "print(f\"The inertia for ng group data with k = 20 is: {inertia}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0abf4ce7-9d21-4fe0-a44c-aa277471d6fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.KMeans_ng at 0x16b2b6570>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 40\n",
    "kmeans_40 = KMeans_ng(n_clusters = k)\n",
    "kmeans_40.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3921c6be-7056-4e23-85be-4defd558ed68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purity of ng group data for k = 40 is: 0.44014645017510345\n",
      "The gini of ng group for k = 40 is: 0.6990602091060141\n"
     ]
    }
   ],
   "source": [
    "purity, gini = kmeans_40.calculate_purity_gini(kmeans_40.labels, labels)\n",
    "print(f\"The purity of ng group data for k = 40 is: {purity}\")\n",
    "print(f\"The gini of ng group for k = 40 is: {gini}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f98e1a88-949b-4ec3-afd3-d5c7124a20be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The inertia for ng group data with k = 40 is: 510.562605924934\n"
     ]
    }
   ],
   "source": [
    "inertia = kmeans_40.inertia(x)\n",
    "print(f\"The inertia for ng group data with k = 40 is: {inertia}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fadf6c63-7f16-41cf-a694-a163eaced257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.KMeans_ng at 0x16b1ac5f0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 10\n",
    "kmeans_10 = KMeans_ng(n_clusters = k)\n",
    "kmeans_10.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "12fd5e75-d307-4e66-8981-5a9d387fafb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purity of ng group data for k = 10 is: 0.2864798896317521\n",
      "The gini of ng group for k = 10 is: 0.7986714606850498\n"
     ]
    }
   ],
   "source": [
    "purity, gini = kmeans_10.calculate_purity_gini(kmeans_10.labels, labels)\n",
    "print(f\"The purity of ng group data for k = 10 is: {purity}\")\n",
    "print(f\"The gini of ng group for k = 10 is: {gini}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "16424cea-c284-4d13-b5b4-f9de99f2b09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The inertia for ng group data with k = 10 is: 297.07700080309326\n"
     ]
    }
   ],
   "source": [
    "inertia = kmeans_10.inertia(x)\n",
    "print(f\"The inertia for ng group data with k = 10 is: {inertia}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f338c526-1bdc-4137-b15f-67977c143ac5",
   "metadata": {},
   "source": [
    "Task 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28d149d1-3327-4937-b052-33ee644065c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "class SoftKMeans:\n",
    "    def __init__(self, n_clusters=10, beta=1.0, max_iterations=100, tol=1e-4):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.beta = beta\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tol = tol\n",
    "\n",
    "    def fit(self, X):\n",
    "        # take random centroids\n",
    "        n_samples, n_features = X.shape\n",
    "        self.centroids = X[np.random.choice(n_samples, self.n_clusters, replace=False)]\n",
    "\n",
    "        for iteration in range(self.max_iterations):\n",
    "            distances = pairwise_distances(X, self.centroids, metric='euclidean')\n",
    "            \n",
    "            # use the soft kmeans formula\n",
    "            exponentials = np.exp(-self.beta * distances)\n",
    "            self.responsibilities = exponentials / np.sum(exponentials, axis=1, keepdims=True)\n",
    "\n",
    "            # update cebntroids based on weighted means\n",
    "            new_centroids = np.dot(self.responsibilities.T, X) / np.sum(self.responsibilities.T, axis=1, keepdims=True)\n",
    "\n",
    "            # check if converged or not\n",
    "            if np.linalg.norm(new_centroids - self.centroids) < self.tol:\n",
    "                break\n",
    "            self.centroids = new_centroids\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        distances = pairwise_distances(X, self.centroids, metric='euclidean')\n",
    "        exponentials = np.exp(-self.beta * distances)\n",
    "        responsibilities = exponentials / np.sum(exponentials, axis=1, keepdims=True)\n",
    "        return np.argmax(responsibilities, axis=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_purity_gini(labels, true_labels):\n",
    "        from collections import Counter\n",
    "        n_samples = len(labels)\n",
    "        clusters = np.unique(labels)\n",
    "        purity, gini = 0, 0\n",
    "\n",
    "        for cluster in clusters:\n",
    "            indices = np.where(labels == cluster)[0]\n",
    "            cluster_labels = true_labels[indices]\n",
    "            most_common_label, count = Counter(cluster_labels).most_common(1)[0]\n",
    "            purity += count\n",
    "\n",
    "            label_counts = Counter(cluster_labels)\n",
    "            gini_k = 1 - sum((count / len(indices))**2 for count in label_counts.values())\n",
    "            gini += gini_k * len(indices)\n",
    "\n",
    "        purity /= n_samples\n",
    "        gini /= n_samples\n",
    "        return purity, gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "070391e5-b53c-4964-8a0d-99095cc9b87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purity and gini for the fashion dataset using soft kmeans for k = 10 is 0.2583, 0.7847452538797828\n"
     ]
    }
   ],
   "source": [
    "soft_kmeans = SoftKMeans(n_clusters = 10)\n",
    "soft_kmeans.fit(x_data)\n",
    "pred = soft_kmeans.predict(x_data)\n",
    "purity, gini = soft_kmeans.calc_purity_gini(pred, y_data)\n",
    "print(f\"The purity and gini for the fashion dataset using soft kmeans for k = 10 is {purity}, {gini}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2bec09-3a12-4dcf-9301-e15d843b36be",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468ca3db-43cd-41e0-b0fe-7ee3ad1b5d87",
   "metadata": {},
   "source": [
    "Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "64f9d71c-6434-4b12-99cc-8a8cbe2671f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration 32\n",
      "Estimated Means:\n",
      "[[7.01304477 3.98307506]\n",
      " [2.99393262 3.05212028]]\n",
      "\n",
      "Estimated Covariances:\n",
      "[[[0.97494052 0.49757271]\n",
      "  [0.49757271 1.00123008]]\n",
      "\n",
      " [[1.00990629 0.02722722]\n",
      "  [0.02722722 2.93797867]]]\n",
      "\n",
      "Estimated Mixing Coefficients:\n",
      "[0.66523796 0.33476204]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "data = np.loadtxt(\"2gaussian.txt\") \n",
    "K = 2\n",
    "n, d = data.shape\n",
    "# here we randomly init means, cov and mix coeff\n",
    "means = np.random.rand(K, d) * (np.max(data, axis=0) - np.min(data, axis=0)) + np.min(data, axis=0)\n",
    "covariances = np.array([np.eye(d) for _ in range(K)])\n",
    "mixing_coefficients = np.ones(K) / K \n",
    "\n",
    "\n",
    "max_iter = 100  \n",
    "tolerance = 1e-4  \n",
    "log_likelihoods = []\n",
    "\n",
    "for iteration in range(max_iter):\n",
    "    # e step\n",
    "    responsibilities = np.zeros((n, K))\n",
    "    for k in range(K):\n",
    "        responsibilities[:, k] = mixing_coefficients[k] * multivariate_normal.pdf(data, mean=means[k], cov=covariances[k])\n",
    "    responsibilities /= np.sum(responsibilities, axis=1, keepdims=True)  # Normalize responsibilities\n",
    "\n",
    "    # m step\n",
    "    N_k = np.sum(responsibilities, axis=0) \n",
    "    for k in range(K):\n",
    "        means[k] = np.sum(responsibilities[:, k][:, np.newaxis] * data, axis=0) / N_k[k]\n",
    "        diff = data - means[k]\n",
    "        covariances[k] = (responsibilities[:, k][:, np.newaxis] * diff).T @ diff / N_k[k]\n",
    "        covariances[k] += 1e-6 * np.eye(d)\n",
    "\n",
    "    mixing_coefficients = N_k / n\n",
    "    log_likelihood = np.sum(\n",
    "        np.log(\n",
    "            np.sum(\n",
    "                [mixing_coefficients[k] * multivariate_normal.pdf(data, mean=means[k], cov=covariances[k]) for k in range(K)],\n",
    "                axis=0\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    log_likelihoods.append(log_likelihood)\n",
    "\n",
    "    if iteration > 0 and abs(log_likelihoods[-1] - log_likelihoods[-2]) < tolerance:\n",
    "        print(f\"Converged at iteration {iteration}\")\n",
    "        break\n",
    "\n",
    "print(\"Estimated Means:\")\n",
    "print(means)\n",
    "print(\"\\nEstimated Covariances:\")\n",
    "print(covariances)\n",
    "print(\"\\nEstimated Mixing Coefficients:\")\n",
    "print(mixing_coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e0f025-8b12-4f6f-910b-7bf380160801",
   "metadata": {},
   "source": [
    "Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6e3c868-d567-4aff-8674-a37110a51888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration 54\n",
      "Estimated Means:\n",
      "[[7.02145618 4.01540394]\n",
      " [3.03919609 3.04707698]\n",
      " [5.01147955 7.00125092]]\n",
      "\n",
      "Estimated Covariances:\n",
      "[[[0.99059269 0.50102805]\n",
      "  [0.50102805 0.9956578 ]]\n",
      "\n",
      " [[1.02812057 0.02589962]\n",
      "  [0.02589962 3.38185022]]\n",
      "\n",
      " [[0.98001809 0.18534817]\n",
      "  [0.18534817 0.97484192]]]\n",
      "\n",
      "Estimated Mixing Coefficients:\n",
      "[0.29845025 0.20548081 0.49606894]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "data = np.loadtxt(\"3gaussian.txt\") \n",
    "K = 3\n",
    "n, d = data.shape\n",
    "# here we randomly init means, cov and mix coeff\n",
    "means = np.random.rand(K, d) * (np.max(data, axis=0) - np.min(data, axis=0)) + np.min(data, axis=0)\n",
    "covariances = np.array([np.eye(d) for _ in range(K)])\n",
    "mixing_coefficients = np.ones(K) / K \n",
    "\n",
    "\n",
    "max_iter = 100  \n",
    "tolerance = 1e-4  \n",
    "log_likelihoods = []\n",
    "\n",
    "for iteration in range(max_iter):\n",
    "    # e step\n",
    "    responsibilities = np.zeros((n, K))\n",
    "    for k in range(K):\n",
    "        responsibilities[:, k] = mixing_coefficients[k] * multivariate_normal.pdf(data, mean=means[k], cov=covariances[k])\n",
    "    responsibilities /= np.sum(responsibilities, axis=1, keepdims=True)  # Normalize responsibilities\n",
    "\n",
    "    # m step\n",
    "    N_k = np.sum(responsibilities, axis=0) \n",
    "    for k in range(K):\n",
    "        means[k] = np.sum(responsibilities[:, k][:, np.newaxis] * data, axis=0) / N_k[k]\n",
    "        diff = data - means[k]\n",
    "        covariances[k] = (responsibilities[:, k][:, np.newaxis] * diff).T @ diff / N_k[k]\n",
    "        covariances[k] += 1e-6 * np.eye(d)\n",
    "\n",
    "    mixing_coefficients = N_k / n\n",
    "    log_likelihood = np.sum(\n",
    "        np.log(\n",
    "            np.sum(\n",
    "                [mixing_coefficients[k] * multivariate_normal.pdf(data, mean=means[k], cov=covariances[k]) for k in range(K)],\n",
    "                axis=0\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    log_likelihoods.append(log_likelihood)\n",
    "\n",
    "    if iteration > 0 and abs(log_likelihoods[-1] - log_likelihoods[-2]) < tolerance:\n",
    "        print(f\"Converged at iteration {iteration}\")\n",
    "        break\n",
    "\n",
    "print(\"Estimated Means:\")\n",
    "print(means)\n",
    "print(\"\\nEstimated Covariances:\")\n",
    "print(covariances)\n",
    "print(\"\\nEstimated Mixing Coefficients:\")\n",
    "print(mixing_coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f0bf25-a91a-4e92-b8ce-aedf94d4fef8",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c921f021-2d33-4aac-b36c-039515b5a966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import binom\n",
    "\n",
    "def load_data(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = np.array([[int(bit) for bit in line.strip().split()] for line in f])\n",
    "    return np.sum(data, axis=1) # calculate the number of heads oer session\n",
    "\n",
    "def em_algorithm(filename, D=20, max_iters=100, tol=1e-6):\n",
    "    \n",
    "    num_heads = load_data(filename)\n",
    "    N = 50\n",
    "    pi = np.array([1/3, 1/3, 1/3])  \n",
    "    p = np.array([0.4, 0.5, 0.6])  #common initial values instead of random for optimised convergence\n",
    "    \n",
    "    for _ in range(max_iters):\n",
    "        responsibilities = np.zeros((N, 3))\n",
    "\n",
    "        for i in range(3):  \n",
    "            likelihood = binom.pmf(num_heads, D, p[i]) \n",
    "            responsibilities[:, i] = pi[i] * likelihood  \n",
    "\n",
    "        responsibilities /= responsibilities.sum(axis=1, keepdims=True)  # normalize\n",
    "        Nk = responsibilities.sum(axis=0) #calculating resp for each coin\n",
    "        pi = np.clip(Nk / N, 1e-3, 1 - 1e-3)  #make sure the values stay within 0 - 1\n",
    "\n",
    "        for i in range(3):\n",
    "            p[i] = np.sum(responsibilities[:, i] * num_heads) / (D * (Nk[i] + 1e-8)) # avoids the zero division error \n",
    "\n",
    "        if np.max(np.abs(pi - Nk / N)) < tol and np.max(np.abs(p - p)) < tol:\n",
    "            break\n",
    "\n",
    "    return pi, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "11caabf3-acd3-43bf-b3af-39ea930c3084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated coin selection probability: [0.32002095 0.21899044 0.46098861]\n",
      "The Estimated head bias probability: [0.28584974 0.53032459 0.74922909]\n"
     ]
    }
   ],
   "source": [
    "file_name = 'coin_flips_outcome.txt'\n",
    "pi_est, p_est = em_algorithm(file_name)\n",
    "print(f\"Estimated coin selection probability: {pi_est}\")\n",
    "print(f\"The Estimated head bias probability: {p_est}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263aa799-c0a5-4fcb-82f3-0ca5e65b0d96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
